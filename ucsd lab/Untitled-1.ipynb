{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the parameters and root URL\n",
    "params = {\n",
    "    'years': list(range(2012, 2025)),\n",
    "    'months': list(range(8, 13)),\n",
    "    'seasons': ['outdoor'],\n",
    "    'level': ['hs']\n",
    "}\n",
    "\n",
    "root_url = \"https://ca.milesplit.com/results/?\"\n",
    "\n",
    "# Generate the URLs\n",
    "urls = [\n",
    "    f\"{root_url}year={year}&month={month}&season={season}&level={level}\"\n",
    "    for year in params['years']\n",
    "    for month in params['months']\n",
    "    for season in params['seasons']\n",
    "    for level in params['level']\n",
    "]\n",
    "\n",
    "# Function to make GET requests with pagination and extract data\n",
    "def fetch_data(url):\n",
    "    page = 1\n",
    "    data = []\n",
    "    while True:\n",
    "        paginated_url = f\"{url}&page={page}\"\n",
    "        response = requests.get(paginated_url, verify=False)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            print(f\"No table found for URL: {paginated_url}\")\n",
    "            break\n",
    "        rows = table.find_all('tr')\n",
    "        if not rows:\n",
    "            print(f\"Table found but it has no content for URL: {paginated_url}\")\n",
    "            break\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            name_element = row.find('td', class_='name')\n",
    "            href_url = name_element.find('a')['href'] if name_element and name_element.find('a') else None\n",
    "            row_data.append(href_url)\n",
    "            data.append({\"url\": paginated_url, \"row_data\": row_data})\n",
    "        page += 1\n",
    "    return data\n",
    "\n",
    "# Initialize a list to store the data\n",
    "all_data = []\n",
    "\n",
    "# Make GET requests for each URL\n",
    "for url in urls:\n",
    "    all_data.extend(fetch_data(url))\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Extract additional columns from the URL\n",
    "df['year'] = df['url'].str.extract(r'year=(\\d+)')\n",
    "df['month'] = df['url'].str.extract(r'month=(\\d+)')\n",
    "df['season'] = df['url'].str.extract(r'season=(\\w+)')\n",
    "df['level'] = df['url'].str.extract(r'level=(\\w+)')\n",
    "df['page'] = df['url'].str.extract(r'page=(\\d+)')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv(\"api_responses_with_tables.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
